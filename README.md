ğŸ¤³CLIPè¿›è¡ŒZERO-SHOTåˆ†ç±»çš„è¿‡ç¨‹ğŸ¤³
---------------------------------------------------------------------------------------------------------------------------------------------------------
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

âœ”ä½¿ç”¨CLIPçš„Zero-Shotåˆ†ç±»è¿‡ç¨‹å›¾
---------------------------------------------------------------------------------------------------------------------------------------------------------
![è®­ç»ƒ](https://user-images.githubusercontent.com/128795948/227481912-f99f75b7-6a3d-4502-a8c4-8a41e0216f18.png)

âœ”CLIPæ¨¡å‹
----------------------------------------------------------------------------------------------------------------------------------------------------------
1.è®­ç»ƒé˜¶æ®µ
CLIPæ¨¡å‹æ¶æ„è¢«åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š  
ï¼ˆ1ï¼‰å›¾åƒç¼–ç å™¨  
ï¼ˆ2ï¼‰æ–‡æœ¬ç¼–ç å™¨ã€‚  

å½“æˆ‘ä»¬å¯¹CLIPæ¨¡å‹æä¾›ä¸€ç»„æ–‡æœ¬æè¿°ï¼Œè¿™äº›æ–‡æœ¬æè¿°ä¼šè¢«ç¼–ç ä¸ºæ–‡æœ¬åµŒå…¥ã€‚  
ç„¶åï¼Œæˆ‘ä»¬å¯¹å›¾åƒé‡å¤è¿™ä¸€è¾“å…¥æ­¥éª¤ï¼Œé‚£ä¹ˆå›¾åƒä¹Ÿä¼šè¢«ç¼–ç æˆå›¾åƒåµŒå…¥ã€‚
  
2.æµ‹è¯•é˜¶æ®µ  
CLIPæ¨¡å‹ä¼šè®¡ç®—å›¾åƒåµŒå…¥å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦ã€‚  
å…¶ä¸­çŸ©é˜µå†…ä½™å¼¦ç›¸ä¼¼åº¦å…·æœ€é«˜çš„æ–‡æœ¬æç¤ºä¼šè¢«é€‰æ‹©ä¸ºé¢„æµ‹ç»“æœï¼Œè¿™æ ·å°±å®Œæˆç›®æ ‡ä»»åŠ¡ä¸Šçš„ zero-shot åˆ†ç±»ã€‚


åŒæ—¶ï¼Œè¾“å…¥ä¹Ÿå¯ä»¥ä¸æ­¢è¾“å…¥ä¸€å¼ å›¾åƒï¼ŒCLIPç¼“å­˜äº†è¾“å…¥çš„æ–‡æœ¬åµŒå…¥ï¼Œæ‰€ä»¥å®ƒä»¬ä¸å¿…ä¸ºå…¶ä½™çš„è¾“å…¥å›¾åƒé‡æ–°è¿›è¡Œè®¡ç®—ã€‚  
è¿™æ˜¯CLIPç«¯åˆ°ç«¯çš„å·¥ä½œæ–¹å¼ã€‚  

---------------------------------------------------------------------------------------------------------------------------------------------------------

åœ¨è¿™æ ·çš„æ¨¡å‹ä¸­ï¼Œå›¾åƒç¼–ç å™¨é€šå¸¸ç”¨æ¥æå–å›¾åƒçš„ç‰¹å¾å‘é‡ï¼Œè€Œè‡ªç„¶è¯­è¨€ç¼–ç å™¨åˆ™ç”¨æ¥å°†è‡ªç„¶è¯­è¨€åºåˆ—è½¬æ¢ä¸ºå¯ä»¥è¢«è®¡ç®—æœºå¤„ç†çš„å‘é‡ã€‚  
ä½¿ç”¨å›¾åƒç¼–ç å™¨å’Œè‡ªç„¶è¯­è¨€ç¼–ç å™¨æ¥é¢„æµ‹ä¸€æ®µè‡ªç„¶è¯­è¨€æ–‡æœ¬å¯¹åº”çš„å›¾åƒæ—¶ï¼Œæˆ‘ä»¬ä¼šé¦–å…ˆå°†å›¾åƒè¾“å…¥åˆ°å›¾åƒç¼–ç å™¨ä¸­ï¼Œå¾—åˆ°ä¸€ä¸ªå‘é‡ VImageã€‚  
ç„¶åï¼Œæˆ‘ä»¬å°†éœ€è¦é¢„æµ‹çš„æ–‡æœ¬è¾“å…¥åˆ°è‡ªç„¶è¯­è¨€ç¼–ç å™¨ä¸­ï¼Œå¾—åˆ°å¦ä¸€ä¸ªå‘é‡ VLanguageã€‚  

å…¶ä¸­ï¼Œ VLanguageæ˜¯ç”±æˆ‘ä»¬è¾“å…¥çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬åºåˆ—æ‰€äº§ç”Ÿçš„ï¼Œè¿™äº›è¾“å…¥å¯ä»¥æ˜¯å•è¯ã€å¥å­ã€æ®µè½ã€æ–‡æ¡£æˆ–å…¶ä»–åŸºäºæ–‡æœ¬çš„å½¢å¼ï¼Œå¹¶é€šå¸¸ç”±åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿæˆ–ç±»ä¼¼çš„ä»»åŠ¡ä¸­çš„æ–‡æœ¬è¾“å…¥å¾—åˆ°ã€‚  

----------------------------------------------------------------------------------------------------------------------------------------------------------

æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œè‡ªç„¶è¯­è¨€ç¼–ç å™¨é€šå¸¸ç”±å¤šä¸ªå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å±‚æ„æˆï¼Œå®ƒä»¬å¯ä»¥æ¥å—ä¸€ä¸ªè‡ªç„¶è¯­è¨€åºåˆ—ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªé’ˆå¯¹è¯¥åºåˆ—çš„ç‰¹å¾å‘é‡ã€‚è¿™ä¸ªç‰¹å¾å‘é‡é€šå¸¸è¢«ç§°ä¸ºâ€œä¸Šä¸‹æ–‡å‘é‡â€ï¼Œå› ä¸ºå®ƒåŒ…å«äº†å…³äºåºåˆ—ä¸­æ¯ä¸ªè¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ 

å› æ­¤ï¼Œå½“æˆ‘ä»¬è¾“å…¥ä¸€ä¸ªè‡ªç„¶è¯­è¨€åºåˆ—åˆ°è‡ªç„¶è¯­è¨€ç¼–ç å™¨ä¸­æ—¶ï¼Œæ¯ä¸ªè¯ä¼šè¢«ä¾æ¬¡è¾“å…¥åˆ° RNN ä¸­ï¼Œå¹¶ä¸”æ¯ä¸ª RNN å±‚éƒ½ä¼šè¾“å‡ºä¸€ä¸ªä¸­é—´çŠ¶æ€ï¼Œç”¨äºåœ¨åç»­çš„è®¡ç®—ä¸­ç»§ç»­ä¼ é€’ã€‚ 

æœ€ç»ˆï¼Œè‡ªç„¶è¯­è¨€ç¼–ç å™¨è¾“å‡ºçš„å‘é‡ VLanguage å°±æ˜¯é€šè¿‡åœ¨è¿™äº›ä¸­é—´çŠ¶æ€ä¸Šè¿›è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ°çš„ï¼Œè¿™æ ·å¯ä»¥æ•æ‰åˆ°æ•´ä¸ªæ–‡æœ¬åºåˆ—çš„ç‰¹å¾ä¿¡æ¯ï¼Œè€Œä¸ä»…æ˜¯æ¯ä¸ªå•ç‹¬çš„è¯è¯­ä¿¡æ¯ã€‚  

å®ƒåŒ…å«äº†é’ˆå¯¹è¯¥åºåˆ—çš„æ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯ä»¥ç”¨äºé€šè¿‡ä¸å›¾åƒç¼–ç å™¨å¾—åˆ°çš„å‘é‡ VImage è¿›è¡Œè”åˆè®¡ç®—ï¼Œå¾—åˆ°æˆ‘ä»¬éœ€è¦çš„é¢„æµ‹ç»“æœã€‚

----------------------------------------------------------------------------------------------------------------------------------------------------------

âœ”ç¯å¢ƒé…ç½®è¿‡ç¨‹ï¼š
----------------------------------------------------------------------------------------------------------------------------------------------------------

é¦–å…ˆå®ŒæˆAnacondaã€CUDAç­‰çš„é€‚é…åï¼Œå†è¿›è¡Œç¯å¢ƒé€‚é…ã€‚

condaæ·»åŠ å›½å†…æº:

    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/

    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/

    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/

æ˜¾ç¤ºæ·»åŠ çš„æºï¼š

    conda config --show channels

pip æ¢æºï¼š

    pip config set global.index-url https://pypi.douban.com/simple/

ç„¶ååˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œæ‰“å¼€Anaconda PowerShell Promptè¾“å…¥ï¼š
  
      conda create -n {torch.ç‰ˆæœ¬} python={ç‰ˆæœ¬}ï¼Œ
  
  åˆ›å»ºæˆåŠŸåï¼Œæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ:
  
      conda activate torch1.1
  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

âœ”æµ‹è¯•æ“ä½œè¿‡ç¨‹
----------------------------------------------------------------------------------------------------------------------------------------------------------

ä»githubå…‹éš†ä¸‹CLIP-mainé¡¹ç›®åï¼Œä¼ è¾“å…¥è‡ªå·±çš„anacondaè™šæ‹Ÿç©ºé—´ä¸­ï¼Œåœ¨æ–‡ä»¶å¤¹ä¸‹åˆ›å»ºè‡ªå·±çš„pyé¡¹ç›®å¦‚ï¼šEnvelope(Pred).pyç­‰è¯¸å¦‚æ­¤ç±»ã€‚

åœ¨pycharmé‡Œæ‰“å¼€è‡ªå·±æ–°å»ºçš„æ–‡ä»¶é¡¹ç›®ï¼Œå¯¼å…¥ä½ çš„ä»£ç ã€‚

    import torch
    import clip
    from PIL import Image  

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)  
    
    image = preprocess(Image.open("CLIP.png")).unsqueeze(0).to(device)
    text = clip.tokenize(["a diagram", "a dog", "a cat"]).to(device)  

    with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)  
    
    logits_per_image, logits_per_text = model(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()  
    
    print("Label probs:", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]  

ï¼ˆæ­¤ä»£ç æ—¨åœ¨è°ƒæ•´pytorchä¸torchvisionä¸è®¡ç®—æœºä¸Šçš„ CUDA ç‰ˆæœ¬é€‚é…ï¼Œæˆ–è€…æ˜¯åœ¨æ²¡æœ‰ GPU çš„è®¡ç®—æœºä¸Šè¿›è¡Œå®‰è£…æ—¶ä½¿ç”¨ï¼‰

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

âœ”Zero-Shoté¢„æµ‹çº¢åŒ…å›¾ç‰‡ä»¥åŠæ›´å¤šæ‹“å±•
----------------------------------------------------------------------------------------------------------------------------------------------------------

ä¸‹é¢çš„ä»£ç ä½¿ç”¨ CLIP æ‰§è¡Œé›¶é•œå¤´é¢„æµ‹ï¼Œåœ¨æ‰§è¡Œå‰åœ¨ä»»æ„å¹³å°é€‰æ‹©ä¸€å¼ çº¢åŒ…å›¾ç‰‡å¯¼å…¥CLIP-mainè™šæ‹Ÿç©ºé—´æ–‡ä»¶å¤¹ä¸­ï¼Œå¹¶å†æ¬¡æ–°å»ºä¸€ä¸ªpyæ–‡ä»¶ï¼Œè¾“å…¥ä»£ç ï¼š

    import os
    import clip
    import torch
    from PIL import Image
    
    from torchvision.datasets import CIFAR100  

    # Load the model
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load('ViT-B/32', device)  


    # Prepare the inputs
    image = preprocess(Image.open("OP.jfif")).unsqueeze(0).to(device)
    a_list = ['car','rubbish','father','friend']
    text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in a_list]).to(device)  
    
    # Calculate features
    with torch.no_grad():  
    image_features = model.encode_image(image)
    text_features = model.encode_text(text_inputs)  

    # Pick the top 5 most similar labels for the image
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    valuesï¼Œindices = similarity[0].topk(1)  

    # Print the result
    print("\nTop predictions:\n")
    for value, index in zip(values, indices):
    print(f"{a_list[index]:>16s}: {100 * value.item():.2f}%")  
    
æµ‹è¯•å®Œæˆä¹‹åï¼Œä½ åº”è¯¥èƒ½å¾—åˆ°ä¸€ä¸ªé¢„æµ‹å€¼--->
![1](https://user-images.githubusercontent.com/128795948/227507990-8ea63d88-5241-4b9c-ab08-c68ebeef31d7.PNG)
  
è€Œå½“è¾“å…¥çš„è‡ªç„¶è¯­è¨€ä¸­æœ‰red envelopeè¿™ä¸ªå¤åˆè¯æ¡åï¼ŒCLIPåœ¨red envelopeçš„é¢„æµ‹å€¼ç”šè‡³ç›´æ¥è¾¾åˆ°99.92%  
![4](https://user-images.githubusercontent.com/128795948/227509984-3dece91e-859b-4878-9f1d-d4cd59f0ffcf.PNG)

----------------------------------------------------------------------------------------------------------------------------------------------------------

ğŸ‘€æ‹“å±•ï¼šç°åœ¨å°è¯•æ–°å»ºä¸€ä¸ªè¯†åˆ«æ¯å­çš„äºŒåˆ†ç±»ä»»åŠ¡
----------------------------------------------------------------------------------------------------------------------------------------------------------

é‡å¤ä¸Šè¿°æ“ä½œå¯¼å…¥imageæ•°æ®ï¼Œå»ºç«‹è‡ªå·±çš„pyæ–‡ä»¶ï¼Œå¹¶æ‰“å¼€Pycharmè¿›è¡Œæ“ä½œï¼Œä¹‹å‰è¯†åˆ«çº¢åŒ…çš„ä»£ç åŸºæœ¬æ”¹å˜classeså°±èƒ½è¯†åˆ«å¦ä¸€ä¸ªç›®æ ‡ã€‚  

    import os
    import clip
    import torch
    from torchvision.datasets import CIFAR100
    from PIL import Image  

    img_pah = 'CUP.jfif'
    classes = ['cup', 'not cup']  

    # Load the model
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load('ViT-B/32', device)  


    # Prepare the inputs
    image = Image.open(img_pah)
    image_input = preprocess(image).unsqueeze(0).to(device)
    text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in classes]).to(device) #ç”Ÿæˆæ–‡å­—æè¿°  

    # Calculate features
    with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)  

    # Pick the top 5 most similar labels for the image
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1) #å¯¹å›¾åƒæè¿°å’Œå›¾åƒç‰¹å¾
    values, indices = similarity[0].topk(1)  

    # Print the result
    print("\nTop predictions:\n")
    print('classes:{} score:{:.2f}'.format(classes[indices.item()], values.item()))  

å¯¼å…¥å›¾ç‰‡åï¼Œç¼–è¾‘ä»£ç è¾“å…¥è‡ªç„¶è¯­è¨€ä¸ºcupä¸not_cupè¿›è¡Œé¢„æµ‹å¾—åˆ°ç»“æœï¼š

![2](https://user-images.githubusercontent.com/128795948/227509510-70836c6f-ca31-4fc2-a8ca-b00abbe99f5c.PNG)  

æ³¨æ„ï¼šCLIPåœ¨é¢„æµ‹ä¸­æ˜“å—åˆ°å¤šä¹‰è¯çš„å½±å“ã€‚æœ‰æ—¶ï¼Œç”±äºç¼ºä¹ä¸Šä¸‹æ–‡ï¼Œè¯¥æ¨¡å‹æ— æ³•åŒºåˆ†ä¸€äº›è¯çš„å«ä¹‰ï¼Œæ‰€ä»¥åœ¨è¿›è¡Œé¢„æµ‹æ—¶è¾“å…¥çš„è‡ªç„¶è¯­è¨€å¯¹é¢„æµ‹å€¼å°†ä¼šæœ‰å¾ˆå¤§å½±å“ã€‚  
åŒæ—¶CLIPåœ¨æ‰‹å†™æ•°å­—è¯†åˆ«ä»»åŠ¡ä¸Šå¾ˆåƒåŠ›ï¼Œå› ä¸ºå…¶è®­ç»ƒæ•°æ®é›†ä¸­ç¼ºä¹æ‰‹å†™æ•°å­—ã€‚

  â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
